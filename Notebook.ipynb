{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction and Dataset\n",
    "asdf"
   ],
   "id": "6e420e369e1a8334"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-12T23:18:44.735755Z",
     "start_time": "2024-11-12T23:18:44.720069Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Train and Test are used for training/hyperparameter tuning\n",
    "# Validation set is used exclusively for set validation and comparison\n",
    "\n",
    "# Using the static split so that the results are comparable with the paper being referenced.\n",
    "\n",
    "train_df = pd.read_table(\"data/train.tsv\", header=None)\n",
    "test_df = pd.read_table(\"data/test.tsv\", header=None)\n",
    "validation_df = pd.read_table(\"data/dev.tsv\", header=None)\n",
    "\n",
    "print(train_df)\n",
    "\n",
    "train_y = train_df[0].to_numpy()\n",
    "train_x = train_df[1].to_numpy()\n",
    "\n",
    "test_y = test_df[0].to_numpy()\n",
    "test_x = test_df[1].to_numpy()\n",
    "\n",
    "validation_y = validation_df[0].to_numpy()\n",
    "validation_x = validation_df[1].to_numpy()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0                                                  1\n",
      "0     1  a stirring , funny and finally transporting re...\n",
      "1     0  apparently reassembled from the cutting-room f...\n",
      "2     0  they presume their audience wo n't sit still f...\n",
      "3     1  this is a visually stunning rumination on love...\n",
      "4     1  jonathan parker 's bartleby should have been t...\n",
      "...  ..                                                ...\n",
      "6915  1  painful , horrifying and oppressively tragic ,...\n",
      "6916  0  take care is nicely performed by a quintet of ...\n",
      "6917  0  the script covers huge , heavy topics in a bla...\n",
      "6918  0  a seriously bad film with seriously warped log...\n",
      "6919  1  a deliciously nonsensical comedy about a city ...\n",
      "\n",
      "[6920 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T20:49:36.486073Z",
     "start_time": "2024-11-12T20:49:36.483507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_x)\n",
    "print(train_y)\n",
    "\n",
    "print(test_x)\n",
    "print(test_y)"
   ],
   "id": "5bacb59eecb08503",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films'\n",
      " 'apparently reassembled from the cutting-room floor of any given daytime soap .'\n",
      " \"they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\"\n",
      " ...\n",
      " \"the script covers huge , heavy topics in a bland , surfacey way that does n't offer any insight into why , for instance , good things happen to bad people .\"\n",
      " 'a seriously bad film with seriously warped logic by writer-director kurt wimmer at the screenplay level .'\n",
      " 'a deliciously nonsensical comedy about a city coming apart at its seams .']\n",
      "[1 0 0 ... 0 0 1]\n",
      "['no movement , no yuks , not much of anything .'\n",
      " \"a gob of drivel so sickly sweet , even the eager consumers of moore 's pasteurized ditties will retch it up like rancid crème brûlée .\"\n",
      " 'gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up .'\n",
      " ...\n",
      " \"safe conduct , however ambitious and well-intentioned , fails to hit the entertainment bull 's - eye .\"\n",
      " 'a film made with as little wit , interest , and professionalism as artistically possible for a slummy hollywood caper flick .'\n",
      " \"but here 's the real damn : it is n't funny , either .\"]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:54:13.172521Z",
     "start_time": "2024-11-12T23:54:13.166492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk #import the natural language toolkit\n",
    "\n",
    "nltk.download('punkt') #download the package in nltk which supports tokenization\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords') #download the nltk package for stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize #import the tokenize package\n",
    "from nltk.corpus import stopwords, wordnet  # import the package from the corpus\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer  # import the snowball stemmer (also known as Porter2)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "print(stopwords.words(\"english\"))\n",
    "\n",
    "#https://medium.com/@maleeshadesilva21/preprocessing-steps-for-natural-language-processing-nlp-a-beginners-guide-d6d9bf7689c9\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "class PreProcessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        # TODO: Find words that appear in > 50% of the examples. and words appearing in 1-2 examples.\n",
    "        #       Add these to the stopwords\n",
    "        self.all_stopwords = stopwords.words(\"english\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # convert POS tag to WordNet format\n",
    "        def get_wordnet_pos(word):\n",
    "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "            tag_dict = {\"J\": wordnet.ADJ,\n",
    "                        \"N\": wordnet.NOUN,\n",
    "                        \"V\": wordnet.VERB,\n",
    "                        \"R\": wordnet.ADV}\n",
    "            return tag_dict.get(tag, wordnet.NOUN)\n",
    "        \n",
    "        prep_text = []\n",
    "        for x in X:\n",
    "            token_text = word_tokenize(x)\n",
    "            #normd_text = [token.lower() for token in token_text if token.isalpha()]\n",
    "    \n",
    "            # Remove stopwords\n",
    "            #swr_text = [token for token in normd_text if token not in self.all_stopwords]\n",
    "    \n",
    "            # Lemmatizer works well, stopwords is minor reduction, normalization is major reduction\n",
    "            prep_text += [[self.lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in token_text]]\n",
    "            #prep_text += [[self.stemmer.stem(word) for word in swr_text]]\n",
    "    \n",
    "        prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
    "        return prep_sentences"
   ],
   "id": "203bc7520a31ec45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T00:08:02.916039Z",
     "start_time": "2024-11-13T00:07:13.592093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# Simply test this shiz works\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, vector_size=300, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = Word2Vec(X, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "    \n",
    "    def _word2vec_rep(self, sentence):\n",
    "        embs = [self.model.wv[word] for word in sentence if word in self.model.wv.index_to_key]\n",
    "        if len(embs) == 0:\n",
    "            return np.zeros(self.vector_size)\n",
    "        sent_emb = np.mean(np.array(embs), axis=0)\n",
    "        return sent_emb\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self._word2vec_rep(doc) for doc in X])\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('prep', PreProcessor()),\n",
    "    #('count', CountVectorizer(max_features=300)),\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    #('vec', Word2VecTransformer()),\n",
    "    #('rep', TfidfTransformer()),\n",
    "    #('mod', KNeighborsClassifier()),\n",
    "    ('mod', MLPClassifier(solver='lbfgs', hidden_layer_sizes=(32,)))\n",
    "])\n",
    "\n",
    "\n",
    "text_clf.fit(train_x, train_y)\n",
    "predictions = text_clf.predict(test_x)\n",
    "acc = accuracy_score(predictions, test_y)\n",
    "print(acc)"
   ],
   "id": "1139ba88d304f368",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880285557386052\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Representation Learning",
   "id": "df0ffe4ed1df73b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP Algorithms",
   "id": "b03624c178851da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "937f8d3062ea3a1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paper Overview",
   "id": "c79881e915c087f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementation of C-LSTM Algorithm",
   "id": "58436f3d4d9b5539"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paper Evaluation",
   "id": "44d879ead9a96f86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
