{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e420e369e1a8334",
   "metadata": {},
   "source": [
    "## Introduction and Dataset\n",
    "TODO: Introduce the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:58:56.240101Z",
     "start_time": "2024-11-20T16:58:55.952824Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Train and Test are used for training/hyperparameter tuning\n",
    "# Validation set is used exclusively for set validation and comparison\n",
    "\n",
    "# Using the static split so that the results are comparable with the paper being referenced.\n",
    "train_df = pd.read_table(\"data/train.tsv\", header=None)\n",
    "test_df = pd.read_table(\"data/test.tsv\", header=None)\n",
    "validation_df = pd.read_table(\"data/dev.tsv\", header=None)\n",
    "\n",
    "print(train_df)\n",
    "\n",
    "train_y = train_df[0].to_numpy()\n",
    "train_x = train_df[1].to_numpy()\n",
    "\n",
    "test_y = test_df[0].to_numpy()\n",
    "test_x = test_df[1].to_numpy()\n",
    "\n",
    "validation_y = validation_df[0].to_numpy()\n",
    "validation_x = validation_df[1].to_numpy()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0                                                  1\n",
      "0     1  a stirring , funny and finally transporting re...\n",
      "1     0  apparently reassembled from the cutting-room f...\n",
      "2     0  they presume their audience wo n't sit still f...\n",
      "3     1  this is a visually stunning rumination on love...\n",
      "4     1  jonathan parker 's bartleby should have been t...\n",
      "...  ..                                                ...\n",
      "6915  1  painful , horrifying and oppressively tragic ,...\n",
      "6916  0  take care is nicely performed by a quintet of ...\n",
      "6917  0  the script covers huge , heavy topics in a bla...\n",
      "6918  0  a seriously bad film with seriously warped log...\n",
      "6919  1  a deliciously nonsensical comedy about a city ...\n",
      "\n",
      "[6920 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "5bacb59eecb08503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:58:56.246957Z",
     "start_time": "2024-11-20T16:58:56.245157Z"
    }
   },
   "source": [
    "print(train_x)\n",
    "print(train_y)\n",
    "\n",
    "print(test_x)\n",
    "print(test_y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films'\n",
      " 'apparently reassembled from the cutting-room floor of any given daytime soap .'\n",
      " \"they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\"\n",
      " ...\n",
      " \"the script covers huge , heavy topics in a bland , surfacey way that does n't offer any insight into why , for instance , good things happen to bad people .\"\n",
      " 'a seriously bad film with seriously warped logic by writer-director kurt wimmer at the screenplay level .'\n",
      " 'a deliciously nonsensical comedy about a city coming apart at its seams .']\n",
      "[1 0 0 ... 0 0 1]\n",
      "['no movement , no yuks , not much of anything .'\n",
      " \"a gob of drivel so sickly sweet , even the eager consumers of moore 's pasteurized ditties will retch it up like rancid crème brûlée .\"\n",
      " 'gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up .'\n",
      " ...\n",
      " \"safe conduct , however ambitious and well-intentioned , fails to hit the entertainment bull 's - eye .\"\n",
      " 'a film made with as little wit , interest , and professionalism as artistically possible for a slummy hollywood caper flick .'\n",
      " \"but here 's the real damn : it is n't funny , either .\"]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "203bc7520a31ec45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:58:57.316140Z",
     "start_time": "2024-11-20T16:58:56.308980Z"
    }
   },
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize #import the tokenize package\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer  # import the snowball stemmer (also known as Porter2)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#https://medium.com/@maleeshadesilva21/preprocessing-steps-for-natural-language-processing-nlp-a-beginners-guide-d6d9bf7689c9\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Based on stopwords.words(\"english\")\n",
    "stopwords_english_tuned = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "class PreProcessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.all_stopwords = stopwords_english_tuned\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Strip out words in 75% or more of the texts\n",
    "        token_counts = {}\n",
    "        for x in X:\n",
    "            tokens = word_tokenize(x)\n",
    "            for token in tokens:\n",
    "                token_counts[token] = token_counts.get(token, 0) + 1\n",
    "\n",
    "        common_threshold = len(X) * 0.75\n",
    "        for token in token_counts:\n",
    "            if token_counts[token] >= common_threshold:\n",
    "                self.all_stopwords.append(token)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def posToWordnetPos(self, pos):\n",
    "        #https://medium.com/@maleeshadesilva21/preprocessing-steps-for-natural-language-processing-nlp-a-beginners-guide-d6d9bf7689c9\n",
    "        tag_mapping = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_mapping.get(pos[0].upper(), wordnet.NOUN)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        prep_text = []\n",
    "        for x in X:\n",
    "            tokens = nltk.pos_tag(word_tokenize(x))\n",
    "            tokens_stripped = [token for token in tokens if token[0] not in self.all_stopwords]\n",
    "            tokens_lemmatized = [[self.lemmatizer.lemmatize(token[0], self.posToWordnetPos(token[1])) for token in tokens_stripped]]\n",
    "            tokens_stemmed = [[self.stemmer.stem(token[0]) for token in tokens_stripped]]\n",
    "            prep_text += tokens_stemmed\n",
    "\n",
    "        prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
    "        return prep_sentences"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/reecemackie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1139ba88d304f368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T19:28:44.135555Z",
     "start_time": "2024-11-20T19:27:52.623623Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "# Simply test this shiz works\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, vector_size=300, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = Word2Vec(X, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "    \n",
    "    def _word2vec_rep(self, sentence):\n",
    "        embs = [self.model.wv[word] for word in sentence if word in self.model.wv.index_to_key]\n",
    "        if len(embs) == 0:\n",
    "            return np.zeros(self.vector_size)\n",
    "        sent_emb = np.mean(np.array(embs), axis=0)\n",
    "        return sent_emb\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self._word2vec_rep(doc) for doc in X])\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('prep', PreProcessor()),\n",
    "    #('hash', HashingVectorizer()),\n",
    "    #('count', CountVectorizer(max_features=300)),\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    #('vec', Word2VecTransformer()),\n",
    "    #('rep', TfidfTransformer()),\n",
    "    #('mod', KNeighborsClassifier()),\n",
    "    ('mod', MLPClassifier(solver='lbfgs', hidden_layer_sizes=(32,)))\n",
    "])\n",
    "\n",
    "text_clf.fit(train_x, train_y)\n",
    "predictions = text_clf.predict(test_x)\n",
    "acc = accuracy_score(test_y, predictions)\n",
    "print(acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7539813289401428\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "df0ffe4ed1df73b6",
   "metadata": {},
   "source": [
    "## Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "id": "c392fe178d17a40d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:59:01.953510Z",
     "start_time": "2024-11-20T16:59:01.951595Z"
    }
   },
   "source": [
    "import random\n",
    "# Need to test evaluate: CountVec, Word2Vec, TfidfVec, Hash and Hash + TfidfTransformer.\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ensure consistency between runs\n",
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed = seed\n",
    "\n",
    "# Prepare to measure performance between rep. learning methods\n",
    "accuracy_scores = {}\n",
    "f1_scores = {}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6c6b6a40dcc933df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:59:19.175366Z",
     "start_time": "2024-11-20T16:59:01.960890Z"
    }
   },
   "source": [
    "# Count Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    (\"pre\", PreProcessor()),\n",
    "    (\"vec\", CountVectorizer()),\n",
    "    (\"clf\", KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    #\"vec__max_features\": [100, 200, 300, 400, 500, 600],\n",
    "    \"vec__max_features\": [400],\n",
    "}\n",
    "\n",
    "count_grid = GridSearchCV(count_pipeline, param_grid)\n",
    "count_grid.fit(train_x, train_y)\n",
    "\n",
    "print(count_grid.best_params_)\n",
    "\n",
    "predictions = count_grid.predict(test_x)\n",
    "accuracy_scores[\"count\"] = accuracy_score(test_y, predictions)\n",
    "f1_scores[\"count\"] = f1_score(test_y, predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vec__max_features': 400}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ea2805ab767ec113",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T23:31:20.257500Z",
     "start_time": "2024-11-20T22:43:12.548113Z"
    }
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, vector_size=300, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = Word2Vec(X, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "\n",
    "    def _word2vec_rep(self, sentence):\n",
    "        embs = [self.model.wv[word] for word in sentence if word in self.model.wv.index_to_key]\n",
    "        if len(embs) == 0:\n",
    "            return np.zeros(self.vector_size)\n",
    "        sent_emb = np.mean(np.array(embs), axis=0)\n",
    "        return sent_emb\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self._word2vec_rep(doc) for doc in X])\n",
    "\n",
    "w2v_pipeline = Pipeline([\n",
    "    (\"pre\", PreProcessor()),\n",
    "    (\"vec\", Word2VecTransformer()),\n",
    "    #(\"clf\", KNeighborsClassifier()),\n",
    "    (\"clf\", MLPClassifier(solver='lbfgs', hidden_layer_sizes=(64,32,), max_iter=800))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    #\"vec__vector_size\": [100, 200, 300, 400],\n",
    "    #\"vec__window\": [2, 3, 5],\n",
    "    #\"vec__min_count\": [1,2,3],\n",
    "    \"vec__vector_size\": [300],\n",
    "    \"vec__window\": [5],\n",
    "    \"vec__min_count\": [1],\n",
    "}\n",
    "\n",
    "w2v_grid = GridSearchCV(w2v_pipeline, param_grid)\n",
    "w2v_grid.fit(train_x, train_y)\n",
    "\n",
    "print(w2v_grid.best_params_)\n",
    "\n",
    "predictions = w2v_grid.predict(test_x)\n",
    "accuracy_scores[\"w2v\"] = accuracy_score(test_y, predictions)\n",
    "f1_scores[\"w2v\"] = f1_score(test_y, predictions)\n",
    "\n",
    "print(accuracy_scores[\"w2v\"])\n",
    "print(f1_scores[\"w2v\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reecemackie/Projects/Rover656/CMM307-AdvancedAI/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/reecemackie/Projects/Rover656/CMM307-AdvancedAI/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/reecemackie/Projects/Rover656/CMM307-AdvancedAI/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/reecemackie/Projects/Rover656/CMM307-AdvancedAI/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/reecemackie/Projects/Rover656/CMM307-AdvancedAI/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/reecemackie/Projects/Rover656/CMM307-AdvancedAI/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vec__min_count': 1, 'vec__vector_size': 300, 'vec__window': 5}\n",
      "0.5606809445359693\n",
      "0.5820271682340648\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "id": "10645348a2e4c6c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:59:57.064478Z",
     "start_time": "2024-11-20T16:59:39.388402Z"
    }
   },
   "source": [
    "# TF-IDF Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    (\"pre\", PreProcessor()),\n",
    "    (\"vec\", TfidfVectorizer()),\n",
    "    (\"clf\", KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    #\"vec__lowercase\": [True, False],\n",
    "    #\"vec__max_features\": [100, 200, 300, 400],\n",
    "    #\"vec__ngram_range\": [(1, 1), (1,2), (2,2)],\n",
    "    #\"vec__norm\": [None, \"l1\", \"l2\"],\n",
    "    \"vec__lowercase\": [True],\n",
    "    \"vec__max_features\": [200],\n",
    "    \"vec__ngram_range\": [(1,2)],\n",
    "    \"vec__norm\": [\"l2\"],\n",
    "}\n",
    "\n",
    "tfidf_grid = GridSearchCV(tfidf_pipeline, param_grid)\n",
    "tfidf_grid.fit(train_x, train_y)\n",
    "\n",
    "print(tfidf_grid.best_params_)\n",
    "\n",
    "predictions = tfidf_grid.predict(test_x)\n",
    "accuracy_scores[\"tfidf\"] = accuracy_score(test_y, predictions)\n",
    "f1_scores[\"tfidf\"] = f1_score(test_y, predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vec__lowercase': True, 'vec__max_features': 200, 'vec__ngram_range': (1, 2), 'vec__norm': 'l2'}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "a6dd127a586b37e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:00:14.291642Z",
     "start_time": "2024-11-20T16:59:57.076626Z"
    }
   },
   "source": [
    "# Hashing Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hash_vectorizer_pipeline = Pipeline([\n",
    "    (\"pre\", PreProcessor()),\n",
    "    (\"hash\", HashingVectorizer()),\n",
    "    (\"clf\", KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    #\"hash__lowercase\": [True, False],\n",
    "    #\"hash__strip_accents\": [\"unicode\", \"ascii\"],\n",
    "    #\"hash__norm\": [None, \"l1\", \"l2\"],\n",
    "    \"hash__lowercase\": [True],\n",
    "    \"hash__strip_accents\": [\"unicode\"],\n",
    "    \"hash__norm\": [\"l2\"],\n",
    "}\n",
    "\n",
    "hash_grid = GridSearchCV(hash_vectorizer_pipeline, param_grid)\n",
    "hash_grid.fit(train_x, train_y)\n",
    "\n",
    "print(hash_grid.best_params_)\n",
    "\n",
    "predictions = hash_grid.predict(test_x)\n",
    "accuracy_scores[\"hash\"] = accuracy_score(test_y, predictions)\n",
    "f1_scores[\"hash\"] = f1_score(test_y, predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hash__lowercase': True, 'hash__norm': 'l2', 'hash__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "1d8296f7eeee78a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:00:31.745582Z",
     "start_time": "2024-11-20T17:00:14.305615Z"
    }
   },
   "source": [
    "# Hashing Vectorizer + TF-IDF Transformer\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "\n",
    "hash_tfidf_vectorizer_pipeline = Pipeline([\n",
    "    (\"pre\", PreProcessor()),\n",
    "    (\"hash\", HashingVectorizer()),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # \"hash__lowercase\": [True, False],\n",
    "    # \"hash__strip_accents\": [\"unicode\", \"ascii\"],\n",
    "    # \"hash__norm\": [None, \"l1\", \"l2\"],\n",
    "    # \"tfidf__norm\": [None, \"l1\", \"l2\"],\n",
    "    # \"tfidf__use_idf\": [True, False],\n",
    "    # \"tfidf__smooth_idf\": [True, False],\n",
    "    # \"tfidf__sublinear_tf\": [True, False],\n",
    "    \"hash__lowercase\": [True],\n",
    "    \"hash__strip_accents\": [\"unicode\"],\n",
    "    \"hash__norm\": [\"l2\"],\n",
    "    \"tfidf__norm\": [\"l2\"],\n",
    "    \"tfidf__use_idf\": [True],\n",
    "    \"tfidf__smooth_idf\": [True],\n",
    "    \"tfidf__sublinear_tf\": [False],\n",
    "}\n",
    "\n",
    "hash_tfidf_grid = GridSearchCV(hash_tfidf_vectorizer_pipeline, param_grid)\n",
    "hash_tfidf_grid.fit(train_x, train_y)\n",
    "\n",
    "print(hash_tfidf_grid.best_params_)\n",
    "\n",
    "predictions = hash_tfidf_grid.predict(test_x)\n",
    "accuracy_scores[\"hash_tfidf\"] = accuracy_score(test_y, predictions)\n",
    "f1_scores[\"hash_tfidf\"] = f1_score(test_y, predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hash__lowercase': True, 'hash__norm': 'l2', 'hash__strip_accents': 'unicode', 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "9b10bca517856565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T17:00:31.762575Z",
     "start_time": "2024-11-20T17:00:31.761072Z"
    }
   },
   "source": [
    "print(accuracy_scores)\n",
    "print(f1_scores)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 0.5689181768259198, 'w2v': 0.5112575507962658, 'tfidf': 0.6166941241076331, 'hash': 0.6743547501372872, 'hash_tfidf': 0.742998352553542}\n",
      "{'count': 0.5290941811637673, 'w2v': 0.5514112903225806, 'tfidf': 0.6096196868008948, 'hash': 0.6957414058491534, 'hash_tfidf': 0.7513283740701382}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "d152ed0bfbc1a82e",
   "metadata": {},
   "source": [
    "After comparing Word-2-Vector, CountVectorizer, TF-IDF, HashingVectorizer and HashingVectorizer combined with TF-IDF; Hash + TF-IDF resulted in the most performant model when used for kNN classification. As such, this will be the representation used for the remainder of this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03624c178851da7",
   "metadata": {},
   "source": [
    "## NLP Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8bac71ad8b41401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:18:04.200741Z",
     "start_time": "2024-11-23T21:16:29.992701Z"
    }
   },
   "source": [
    "# TODO: implement LSTM and another algorithm\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "max_len = 0\n",
    "for x in train_x:\n",
    "    max_len = max(max_len, len(x))\n",
    "for x in test_x:\n",
    "    max_len = max(max_len, len(x))\n",
    "\n",
    "# Keras input length pre-process\n",
    "class SequencePadding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return sequence.pad_sequences(X.toarray(), maxlen=self.max_len)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((400,)),\n",
    "        #layers.Embedding(input_dim=2**20, output_dim=256),\n",
    "        layers.Embedding(input_dim=400, output_dim=64),\n",
    "        layers.SimpleRNN(400, activation=\"relu\"),\n",
    "        #layers.Dense(256, activation=\"relu\"),\n",
    "        #layers.LSTM(256),\n",
    "        #layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#model.summary()\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "def get_model(hidden_layer_dim, meta):\n",
    "    # note that meta is a special argument that will be\n",
    "    # handed a dict containing input metadata\n",
    "    n_features_in_ = meta[\"n_features_in_\"]\n",
    "    X_shape_ = meta[\"X_shape_\"]\n",
    "    n_classes_ = meta[\"n_classes_\"]\n",
    "\n",
    "    print(meta)\n",
    "    print(n_features_in_)\n",
    "    print(X_shape_)\n",
    "    print(n_classes_)\n",
    "\n",
    "    print(X_shape_[1:])\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(X_shape_[1:]),\n",
    "        #layers.Dense(n_features_in_, activation=\"relu\"),\n",
    "        #layers.Dense(hidden_layer_dim, activation=\"relu\"),\n",
    "\n",
    "        layers.Embedding(n_features_in_, hidden_layer_dim, input_length=max_len),\n",
    "        #layers.Bidirectional(layers.LSTM(hidden_layer_dim)),\n",
    "        layers.SimpleRNN(hidden_layer_dim, activation=\"relu\"),\n",
    "        #layers.Dense(n_features_in_, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "\n",
    "        #layers.Embedding(n_features_in_, 128),\n",
    "        #layers.Dropout(0.5),\n",
    "        #layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n",
    "        #layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n",
    "        #layers.GlobalMaxPooling1D(),\n",
    "        #layers.Dense(128, activation=\"relu\"),\n",
    "        #layers.Dropout(0.5),\n",
    "        #layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    hidden_layer_dim=64,\n",
    "    metrics=[\"accuracy\"],\n",
    "    epochs=10,\n",
    "    #batch_size=batch_size,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"pre\", PreProcessor()),\n",
    "    #(\"hash\", HashingVectorizer()),\n",
    "    #(\"tfidf\", TfidfTransformer()),\n",
    "    #(\"vec\", CountVectorizer(max_features=400)),\n",
    "    #(\"vec\", Word2VecTransformer()),\n",
    "    (\"vec\", TfidfVectorizer(max_features=400)),\n",
    "    #(\"vec\", TfidfVectorizer(max_features=800)),\n",
    "    #(\"pad\", SequencePadding(400)),\n",
    "    (\"clf\", KerasClassifier(model, epochs=epochs, batch_size=batch_size)),\n",
    "    #(\"clf\", KNeighborsClassifier()),\n",
    "    #(\"clf\", MLPClassifier(solver='lbfgs', hidden_layer_sizes=(32,)))\n",
    "    (\"clf\", clf),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "predictions = pipeline.predict(test_x)\n",
    "acc = accuracy_score(test_y, predictions)\n",
    "print(acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes_': array([0, 1]), 'target_type_': 'binary', 'y_dtype_': dtype('int64'), 'y_ndim_': 1, 'X_dtype_': dtype('float32'), 'X_shape_': (6920, 400), 'n_features_in_': 400, 'target_encoder_': ClassifierLabelEncoder(loss='binary_crossentropy'), 'n_classes_': 2, 'n_outputs_': 1, 'n_outputs_expected_': 1, 'feature_encoder_': FunctionTransformer()}\n",
      "400\n",
      "(6920, 400)\n",
      "2\n",
      "(400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reecemackie/Projects/Rover656/CMM307-AdvancedAI/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_164\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_164\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_144 (\u001B[38;5;33mEmbedding\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m400\u001B[0m, \u001B[38;5;34m64\u001B[0m)        │        \u001B[38;5;34m25,600\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_114 (\u001B[38;5;33mSimpleRNN\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m8,256\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_223 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m65\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_144 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_223 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m33,921\u001B[0m (132.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,921</span> (132.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m33,921\u001B[0m (132.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,921</span> (132.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 28ms/step - accuracy: 0.5130 - loss: 0.6933\n",
      "Epoch 2/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 28ms/step - accuracy: 0.5202 - loss: 0.6925\n",
      "Epoch 3/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 28ms/step - accuracy: 0.5172 - loss: 0.6927\n",
      "Epoch 4/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 28ms/step - accuracy: 0.5217 - loss: 0.6923\n",
      "Epoch 5/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 28ms/step - accuracy: 0.5182 - loss: 0.6926\n",
      "Epoch 6/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 28ms/step - accuracy: 0.5129 - loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 29ms/step - accuracy: 0.5289 - loss: 0.6916\n",
      "Epoch 8/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 29ms/step - accuracy: 0.5185 - loss: 0.6925\n",
      "Epoch 9/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 28ms/step - accuracy: 0.5217 - loss: 0.6924\n",
      "Epoch 10/10\n",
      "\u001B[1m217/217\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 29ms/step - accuracy: 0.5203 - loss: 0.6924\n",
      "\u001B[1m57/57\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step\n",
      "0.49917627677100496\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "cell_type": "markdown",
   "id": "937f8d3062ea3a1e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79881e915c087f2",
   "metadata": {},
   "source": [
    "## Paper Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58436f3d4d9b5539",
   "metadata": {},
   "source": [
    "## Implementation of C-LSTM Algorithm\n",
    "- Word2Vec for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d879ead9a96f86",
   "metadata": {},
   "source": [
    "## Paper Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
